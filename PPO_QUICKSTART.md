# ğŸš€ PPO CartPole - Quick Start Guide

Enhanced PPO implementation with model persistence, TensorBoard logging, and production-ready features.

---

## âœ¨ **What's New (Full Version)**

### **Fixed Issues:**
âœ… **done/truncated handling** - Properly checks both termination conditions
âœ… **weights_only=False** - PyTorch 2.9+ compatibility
âœ… **Print interval** - Reduced from 20 to 5 for better feedback
âœ… **numpy tensor conversion** - Efficient array conversion

### **New Features:**
âœ… **Model Persistence** - Auto-save every 5 episodes, resume training
âœ… **TensorBoard Logging** - Real-time metrics visualization
âœ… **Best Model Tracking** - Separate best model saving
âœ… **Solved Detection** - Auto-stop when avg >= 195
âœ… **Advanced Metrics** - Policy loss, value loss, advantage tracking

---

## ğŸ¯ **Quick Start**

### **1. Train PPO**

```bash
python3 ppo.py
```

**Output:**
```
ğŸ†• No checkpoint found. Starting from scratch...
============================================================
ğŸš€ Starting PPO Training on CartPole-v1
============================================================
Episodes: 0 â†’ 2000
Learning Rate: 0.0005
Gamma: 0.98
Lambda (GAE): 0.95
Epsilon Clip: 0.1
K Epochs: 3
T Horizon: 20
Print Interval: 5
TensorBoard: runs/PPO_CartPole_20250118_150000
============================================================

n_episode :    5, score : 22.4, avg_100: 22.4, loss: 0.0123
n_episode :   10, score : 35.8, avg_100: 29.1, loss: 0.0098
...
n_episode :  300, score : 185.2, avg_100: 175.5, loss: 0.0034
ğŸ† NEW BEST MODEL! Score: 185.2 â†’ Saved to checkpoints/ppo_cartpole_best.pth

n_episode :  450, score : 197.5, avg_100: 195.8, loss: 0.0028

ğŸ‰ Environment SOLVED in 450 episodes!
ğŸ‰ Average Score: 195.80
```

### **2. Test the Model**

```bash
# Basic test (10 episodes)
python3 test_ppo.py

# Test 50 episodes
python3 test_ppo.py --episodes 50

# Render (requires display + pygame)
python3 test_ppo.py --render

# Compare best vs latest
python3 test_ppo.py --compare
```

**Output:**
```
ğŸ“‚ Loading model: checkpoints/ppo_cartpole_best.pth
âœ… Model loaded!
   Episode: 450
   Best Score: 197.5

============================================================
ğŸ® Running 10 test episodes...
============================================================

Episode  1: Score =  200.0, Steps = 200
Episode  2: Score =  198.0, Steps = 198
Episode  3: Score =  195.0, Steps = 195
...

============================================================
ğŸ“Š Test Results:
============================================================
Average Score: 197.80 Â± 2.15
Min Score:     195.0
Max Score:     200.0
Success Rate:  100.0%
============================================================

âœ… Model is performing well! (avg >= 195)
```

### **3. View TensorBoard**

```bash
tensorboard --logdir=runs --port=6006
# Open: http://localhost:6006
```

---

## ğŸ“Š **Key Differences: PPO vs DQN**

| Feature | DQN | PPO |
|---------|-----|-----|
| **Algorithm Type** | Off-policy | On-policy |
| **Experience Replay** | âœ… 50K buffer | âŒ No replay |
| **Training Frequency** | Every episode | Every T_horizon steps |
| **Target Network** | âœ… Q-target | âŒ Not needed |
| **Exploration** | Îµ-greedy | Policy entropy |
| **Advantage** | TD error | GAE (Î»=0.95) |
| **Clipping** | None | PPO clip (Îµ=0.1) |
| **Training Speed** | Slower (more stable) | **Faster** âš¡ |
| **Solve Time** | ~800-1200 episodes | **~400-600 episodes** ğŸ† |

**TL;DR:** PPO is usually **faster** and **more sample-efficient** for CartPole!

---

## ğŸ”§ **Hyperparameters Explained**

### **PPO-Specific:**

```python
lmbda = 0.95         # GAE lambda (Generalized Advantage Estimation)
                     # Higher = more bias, less variance
                     # 0.95 is standard

eps_clip = 0.1       # PPO clip parameter
                     # Limits policy update size
                     # Prevents catastrophic forgetting

K_epoch = 3          # Training epochs per batch
                     # How many times to reuse each batch
                     # 3-10 is typical

T_horizon = 20       # Steps before training
                     # Collect 20 steps, then train
                     # Balance: longer = more data, shorter = fresher
```

### **Standard RL:**

```python
learning_rate = 0.0005  # Adam optimizer LR
gamma = 0.98            # Discount factor
```

---

## ğŸ“ **File Structure**

```
minimalRL/
â”œâ”€â”€ ppo.py                     # Full PPO training script â­
â”œâ”€â”€ test_ppo.py                # Testing script
â”œâ”€â”€ PPO_QUICKSTART.md          # This guide
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ ppo_cartpole_latest.pth   # Latest checkpoint (~150 KB)
â”‚   â””â”€â”€ ppo_cartpole_best.pth     # Best model (~150 KB)
â””â”€â”€ runs/
    â””â”€â”€ PPO_CartPole_20250118_150000/  # TensorBoard logs
```

---

## ğŸ“ˆ **TensorBoard Metrics**

### **Available Graphs:**

1. **Score/episode** - Score per episode
2. **Score/average_100** - Rolling 100-episode average
3. **Loss/total** - Total loss (policy + value)
4. **Loss/policy** - Policy loss (actor)
5. **Loss/value** - Value loss (critic)
6. **Value/average** - Average state value
7. **Advantage/mean** - Mean advantage
8. **Advantage/std** - Advantage standard deviation

---

## ğŸ“ **Training Tips**

### **If training is slow:**
```python
# Increase T_horizon for faster training
T_horizon = 50  # Instead of 20

# Reduce K_epoch for faster updates
K_epoch = 2  # Instead of 3
```

### **If training is unstable:**
```python
# Decrease learning rate
learning_rate = 0.0003

# Increase K_epoch for better optimization
K_epoch = 5

# Stricter clipping
eps_clip = 0.05
```

### **If stuck at plateau:**
```python
# Increase exploration (lower clip)
eps_clip = 0.2

# Adjust GAE lambda
lmbda = 0.9  # More bias, faster learning
```

---

## ğŸ› **Troubleshooting**

### **"RuntimeError: Trying to backward through the graph a second time"**
âœ… Fixed! Added `.detach()` in advantage calculation

### **Episode never ends**
âœ… Fixed! Now checks `done or truncated` (line 221, 229, 234)

### **"FutureWarning: `torch.load` with `weights_only=None`"**
âœ… Fixed! Using `weights_only=False` (line 166)

### **Print interval too slow**
âœ… Fixed! Changed from 20 to 5 episodes (line 197)

---

## ğŸš€ **Expected Performance**

| Episode | Score | Status |
|---------|-------|--------|
| 0-100 | 20-60 | ğŸ”´ Learning basics |
| 100-200 | 60-120 | ğŸŸ¡ Improving |
| 200-400 | 120-180 | ğŸŸ¢ Good progress |
| 400-600 | 180-195 | ğŸŸ¢ Almost solved |
| 600+ | 195+ | âœ… **SOLVED** |

**Typical solve time:** **400-600 episodes** (faster than DQN!)

---

## ğŸ’¡ **Pro Tips**

1. **PPO is sample-efficient** - Solves CartPole faster than DQN
2. **On-policy = no replay buffer** - Uses fresh experiences only
3. **GAE helps** - Reduces variance in advantage estimates
4. **Clipping is crucial** - Prevents too-large policy updates
5. **Watch TensorBoard** - Policy/value loss should decrease together

---

## ğŸ”„ **Continue Training**

```bash
# First run: Episodes 0 â†’ 450 (solved)
python3 ppo.py

# Second run: Loads checkpoint, continues from 450
python3 ppo.py
```

---

## ğŸ¯ **Next Steps**

1. âœ… Train PPO â†’ Run until "SOLVED"
2. âœ… Test performance â†’ Verify avg >= 195
3. âœ… View TensorBoard â†’ Analyze training curves
4. âœ… Compare with DQN â†’ See which is faster
5. âœ… Try other envs â†’ Lunar Lander, Acrobot, etc.

---

## ğŸ¤ **Comparison with DQN**

**When to use PPO:**
- âœ… Need faster training
- âœ… Continuous action spaces
- âœ… Multiple parallel environments
- âœ… More stable policy updates

**When to use DQN:**
- âœ… Discrete actions only
- âœ… Need off-policy learning
- âœ… Want experience replay benefits
- âœ… Atari-like environments

---

Enjoy training! ğŸš€

**Fun fact:** PPO is the algorithm used by OpenAI for training GPT-based systems (RLHF)!
